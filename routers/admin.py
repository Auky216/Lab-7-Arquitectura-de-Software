# routers/admin.py
import time
import random
import asyncio
import structlog
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, Body
from fastapi.security import HTTPBearer
from typing import List
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text

from models.auth import get_current_user
from database.database import get_db, AsyncSessionLocal
from services.cache_service import cache_service
from services.storage_service import storage_service
from services.scraper_service import WebScrapingService
from services.external_apis import ExternalAPIService

admin_router = APIRouter()
security = HTTPBearer()
logger = structlog.get_logger()

@admin_router.post("/ingest/batch")
async def batch_ingest(
    sources: List[str] = Body(..., description="List of sources to ingest from"),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    token: HTTPBearer = Depends(security),
    db: AsyncSession = Depends(get_db)
):
    """Batch ingestion from multiple sources"""
    current_user = await get_current_user(token, db)
    
    if current_user["role"] != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")
    
    job_id = f"batch_{int(time.time())}"
    
    # Schedule background ingestion
    background_tasks.add_task(run_batch_ingestion, sources, job_id)
    
    return {
        "message": "Batch ingestion started",
        "job_id": job_id,
        "sources": sources,
        "estimated_time": f"{len(sources) * 30} seconds"
    }

async def run_batch_ingestion(sources: List[str], job_id: str):
    """Background task for batch ingestion"""
    logger.info("‚öôÔ∏è [BATCH] Starting batch ingestion", job_id=job_id, sources=sources)
    
    results = {}
    
    for source in sources:
        try:
            if source == "arxiv":
                result = await WebScrapingService.scrape_arxiv("cs.AI")
                results[source] = {"status": "success", "papers": len(result["papers"])}
            elif source == "crossref":
                result = await ExternalAPIService.fetch_from_crossref("10.1038/example")
                results[source] = {"status": "success", "papers": 1}
            else:
                results[source] = {"status": "skipped", "reason": "unsupported source"}
        except Exception as e:
            results[source] = {"status": "error", "error": str(e)}
        
        await asyncio.sleep(1)  # Rate limiting
    
    logger.info("‚úÖ [BATCH] Batch ingestion completed", job_id=job_id, results=results)

@admin_router.get("/stats/system")
async def system_statistics(
    token: HTTPBearer = Depends(security),
    db: AsyncSession = Depends(get_db)
):
    """Comprehensive system statistics"""
    current_user = await get_current_user(token, db)
    
    if current_user["role"] != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")
    
    # Get database stats
    async with AsyncSessionLocal() as session:
        papers_count = await session.execute(text("SELECT COUNT(*) FROM papers"))
        users_count = await session.execute(text("SELECT COUNT(*) FROM users"))
        library_items = await session.execute(text("SELECT COUNT(*) FROM library_items"))
    
    return {
        "database": {
            "papers": papers_count.scalar(),
            "users": users_count.scalar(),
            "library_items": library_items.scalar(),
            "size_mb": random.randint(50, 200)
        },
        "cache": cache_service.get_stats(),
        "storage": storage_service.get_storage_stats(),
        "performance": {
            "avg_search_time": "47ms",
            "avg_recommendation_time": "125ms",
            "cache_hit_rate": "85%",
            "uptime": "99.97%"
        },
        "ingestion": {
            "papers_today": random.randint(50, 200),
            "papers_this_week": random.randint(500, 1500),
            "quality_score_avg": 87.3,
            "duplicate_rate": "2.1%"
        },
        "user_activity": {
            "active_users_24h": random.randint(100, 500),
            "searches_24h": random.randint(1000, 5000),
            "downloads_24h": random.randint(200, 800)
        }
    }

@admin_router.get("/cache/stats")
async def cache_statistics(
    token: HTTPBearer = Depends(security),
    db: AsyncSession = Depends(get_db)
):
    """Get cache statistics"""
    current_user = await get_current_user(token, db)
    
    if current_user["role"] != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")
    
    return {
        "cache_stats": cache_service.get_stats(),
        "timestamp": time.time()
    }

@admin_router.post("/cache/clear")
async def clear_cache(
    cache_type: str = Body("all", description="Type of cache to clear: all, search, papers"),
    token: HTTPBearer = Depends(security),
    db: AsyncSession = Depends(get_db)
):
    """Clear cache"""
    current_user = await get_current_user(token, db)
    
    if current_user["role"] != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")
    
    # Mock cache clearing
    cleared_keys = random.randint(10, 100)
    
    logger.info("üßπ [CACHE] Cache cleared", cache_type=cache_type, keys_cleared=cleared_keys)
    
    return {
        "message": f"Cache cleared successfully",
        "cache_type": cache_type,
        "keys_cleared": cleared_keys,
        "timestamp": time.time()
    }